{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12612074,"sourceType":"datasetVersion","datasetId":7967081}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìò Customer Churn Prediction\nA machine learning project to predict customer exit status based on various features such as airline, duration, class, departure time, etc.\n\n","metadata":{}},{"cell_type":"markdown","source":"## üß† Objective\nThe task is to build a predictive model that determines whether a customer of a financial institution will exit or stay (exit_status), based on demographic and transactional features.\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1. üì¶ Importing Libraries\nWe begin by importing essential libraries for data manipulation, visualization, and modeling.","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn==0.24.2\n!pip install imbalanced-learn==0.8.0\n!pip install -U xgboost","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder,MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import precision_score,f1_score,recall_score\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom lightgbm import LGBMClassifier\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom scipy.stats import randint, uniform\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. üóÇÔ∏è Loading Data\nWe load both the training and test datasets and take an initial look.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/customer-exit-status-data/customer_exit_status_data.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. üìä Exploratory Data Analysis (EDA)\nIn this section, we explore the dataset to understand its structure, uncover patterns, and identify any issues such as missing values, duplicates, or outliers.","metadata":{}},{"cell_type":"markdown","source":"### üìê Shape and Structure of Data\n\nData Shape: (90000 rows √ó 14 columns)\n\nThis gives us an idea of dataset size and dimensionality.","metadata":{}},{"cell_type":"code","source":"#Shape of data\n\ndata_shape = data.shape\nprint(f'Shape of training data : {data_shape}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üßæ  Column Names and Data Types\n\nWe inspect the column names and check data types to identify categorical vs. numerical features. This also helps in planning preprocessing and encoding steps.\n\n* Numerical columns: id, customer_id, credit_score, age, tenure, acc_balance, prod_count, has_card, is_active, estimated_salary, salary_status\n\n* Categorical columns : last_name, country, gender","metadata":{}},{"cell_type":"code","source":"#Information on data and column datatypes\n\ndata_info = data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  üìä   Descriptive Statistics\n\nUsing `.describe()`, we look at:\n- Mean, Count, Std, min, max, and percentiles of numerical features like `id`, `credit_score`, `age` etc.The median values are shown in the 50% row.","metadata":{}},{"cell_type":"code","source":"# Descriptive Statistics\n\ndata.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ‚ùó Missing Values\n\nWe check for missing values using `.isnull().sum()`. Handling these is essential before modeling.\n","metadata":{}},{"cell_type":"code","source":"#Missing values\n\nmissing_values = data.isnull().sum()\nprint(f'Number of missing values corresponding to each column :\\n {missing_values}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîÅ Duplicate Records\n\nWe find and count duplicate rows **excluding the `id` column**, which is unique by design.\n\n- Number of duplicated rows found: **0**\n\nDuplicates can distort model training, so we drop them to ensure clean learning.\n","metadata":{}},{"cell_type":"code","source":"#Duplicates\n\nnumber_of_duplicates = data.duplicated(subset=[col for col in data.columns if col != 'id']).sum()\nprint(f'Number of Duplicated Rows : {number_of_duplicates}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìä Boxplot Analysis and Insights\nBoxplots help visualize the distribution of numeric variables and detect outliers. Here's a summary of insights from your boxplots:\n\n‚úÖ Columns with Potential Outliers:\n\n1. `credit_score`:\n\n* Outliers present on the lower end (below ~500).\n\n* Most scores lie between 600‚Äì750.\n\n\n2. `age`\n\n* Significant right-skew with many outliers beyond ~60 years.\n\n* Median age is around early 30.\n\n\n3. `prod_count`\n\n* Outliers present above 3.5.\n\n* Most users have 1‚Äì3 products.\n\n\nüü° Columns with No Major Outliers (Fairly Balanced Distributions):\n\n1. `acc_balance`\n\n* Large spread but no extreme outliers.\n\n* Distribution is positively skewed.\n\n\n2. `estimated_salary`\n\n* Broad range, no major outliers.\n\n* Almost symmetric distribution.\n\n3. `tenure`\n\n* Fairly uniform distribution from 0‚Äì10.\n\n* No significant outliers.","metadata":{}},{"cell_type":"code","source":"#Boxplot\n\nnumeric_cols = data.select_dtypes(include=np.number).columns\nplt.figure(figsize=(15,10))\n\nfor i,col in enumerate(numeric_cols,1):\n    plt.subplot(len(numeric_cols) // 3 + 1, 3, i)\n    sns.boxplot(x=data[col])\n    plt.title(f'Boxplot of {col}')\nplt.tight_layout()\nplt.show()    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîç Correlation Heatmap Analysis\nThe correlation heatmap helps identify relationships between numerical features. Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation). Here's an interpretation of the key findings:\n\n‚úÖ Key Correlation Insights:\n1. `age` vs `exit_status` : 0.34\n\n* Moderate positive correlation.\n\n* Older customers are more likely to exit.\n\n* Strongest correlation with the target ‚Üí important predictive feature.\n\n2. `acc_balance` vs `prod_count` : -0.36\n\n* Negative correlation: customers with higher balance tend to have fewer products.\n\n* Might suggest passive users keeping money without using products.\n\n3. `prod_count` vs `exit_status` : -0.21\n\n* Weak negative correlation.\n\n* Customers with more products are less likely to exit, which is expected.\n\n4. `acc_balance` vs `exit_status` : 0.13\n\n* Slight positive correlation.\n\n* Users with higher account balances may also exit, possibly due to dissatisfaction despite high holdings.\n\nOther features like credit_score, tenure, has_card, is_active, and estimated_salary have very low correlations with exit_status (close to 0), suggesting:\n\n* They are weak individual predictors.\n\n* However, they may still contribute value in multivariate models (e.g., tree-based models).\n\n","metadata":{}},{"cell_type":"code","source":"# Correlation HeatMap\n\nnumeric_cols = data.select_dtypes(include=np.number)\ncorr_matrix = numeric_cols.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr_matrix,cmap='Reds',annot=True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìä Target Variable Distribution: exit_status\n- Label 0: Represents customers who stayed.\n\n- Label 1: Represents customers who exited.\n\nFrom the bar plot:\n\n* About 70,000 customers stayed.\n\n* Only 20,000 customers exited.\n\n‚ö†Ô∏è Class Imbalance Detected\n\n* The dataset is highly imbalanced, with many more examples of class 0 than class 1.\n\n* This imbalance can cause most models to be biased toward predicting the majority class (0), reducing performance for predicting churners (1).","metadata":{}},{"cell_type":"code","source":"# Visualizing the distribution of the target variable (e.g., 'exit_status')\n\nsns.countplot(x='exit_status', data=data, palette='Set1')\nplt.title('Distribution of Target Variable')\nplt.xlabel('Exit Status')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. üßπ Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### ‚úÖ Dropping Irrelevant Columns\n\n* `id`, `customer_id`, `last_name` columns were dropped from the dataset sets as it provides no predictive value.","metadata":{}},{"cell_type":"code","source":"#Dropping irrelevant columns\n\nirr_cols = ['id','customer_id','last_name']\ndata.drop(irr_cols,inplace=True,axis=1)\nprint(f'Train data columns after dropping :\\n{data.columns}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìà Outlier Handling\n* We capped outliers `credit_score` using the IQR method:\n* Outliers were replaced with these bounds to minimize distortion while preserving overall data distribution.","metadata":{}},{"cell_type":"code","source":"print(\"Clipping Outliers in Training dataset\")\ncol='credit_score'\nQ1 = data[col].quantile(0.25)\nQ3 = data[col].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers_count = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()\ndata[col] = np.clip(data[col], lower_bound, upper_bound)\n\nprint(f\"  {col}: {outliers_count} outliers capped between {lower_bound:.2f} and {upper_bound:.2f}\")\n\nplt.figure(figsize=(8,3))\ndata[col].plot(kind='box')\nplt.title(\"Plot after outlier capping\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üéØ Feature & Target Separation \n\nThe target variable for this task is `exit_status`. All other relevant columns were selected as features.","metadata":{}},{"cell_type":"code","source":"#Feature-Target Separation\n\nX = data.drop('exit_status',axis=1)\ny = data['exit_status']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ü™ì Train-Test-Validation Split\n\nThe dataset was split into training, validation and test sets to evaluate model generalization.","metadata":{}},{"cell_type":"code","source":"#Train-Test-Validation Split\n\n# First split: Train (70%) and Temp (30%)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Second split: Validation (15%) and Test (15%)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T15:42:16.350967Z","iopub.execute_input":"2025-07-29T15:42:16.351223Z","iopub.status.idle":"2025-07-29T15:42:16.454357Z","shell.execute_reply.started":"2025-07-29T15:42:16.351197Z","shell.execute_reply":"2025-07-29T15:42:16.453225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üßº Data Preprocessing Pipeline\n\nTo prepare the data for model training, we built a preprocessing pipeline that handles different types of features appropriately:\n\n1. Numerical Columns (Mean Impute + StandardScaler)\n\n\n* For credit_score, acc_balance, and estimated_salary, missing values are imputed using the mean, followed by standard scaling to normalize the distribution.\n\n2. Numerical Columns (Median Impute + MinMaxScaler)\n\n\n* For prod_count, age, and tenure, missing values are filled using the median, then scaled to a 0‚Äì1 range using MinMaxScaler.\n\n3. Categorical Columns\n\n* For country and gender, missing values are imputed using the most frequent category, and then One-Hot Encoded to convert them into numeric form.\n\nColumnTransformer is used to combine all the above steps and apply them to the appropriate columns.\nAny remaining columns (remainder='passthrough') are included without modification.\n\n","metadata":{}},{"cell_type":"code","source":"#Pipeline\n\nnumeric_mean_cols = ['credit_score','acc_balance','estimated_salary']\nnumeric_median_cols = ['prod_count','age','tenure']\ncat_cols = ['country','gender']\n\n\nnumeric_mean_standardscaler = Pipeline([\n    ('mean_impute',SimpleImputer(strategy='mean')),\n    ('standard_Scaler',StandardScaler())\n])\n\nnumeric_median_minmax_scaler = Pipeline([\n    ('median_impute',SimpleImputer(strategy='median')),\n    ('minmax_scaler',MinMaxScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('cat_impute',SimpleImputer(strategy='most_frequent')),\n    ('cat_encode',OneHotEncoder())\n])\n\npreprocessor = ColumnTransformer([\n    ('numeric_mean',numeric_mean_standardscaler,numeric_mean_cols),\n    ('numeric_median',numeric_median_minmax_scaler,numeric_median_cols),\n    ('cat',cat_pipeline,cat_cols)\n],remainder='passthrough')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.üß† Model Building, Tuning, and Evaluation","metadata":{}},{"cell_type":"markdown","source":"### üèóÔ∏è  Model Building\n- A total of **7 different classification models** were trained on the data:\n  - Logistic Regression\n  - AdaBoost\n  - Gaussian Naive Bayes  \n  - Random Forest  \n  - Gradient Boosting  \n  - XGBoost  \n  - LightGBM  \n","metadata":{}},{"cell_type":"code","source":"#Models\n\nmodels={\n    \"RandomForest\":RandomForestClassifier(random_state=42),\n    \"GradientBoost\":GradientBoostingClassifier(),\n    \"XGBoost\":XGBClassifier(),\n    \"LogisticRegression\":LogisticRegression(),\n    \"LightGBM\":LGBMClassifier(random_state=42,verbosity=-1),\n    \"AdaBoost\":AdaBoostClassifier(),\n    \"GaussianNaiveBayes\":GaussianNB(),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìä Model Evaluation\n\nEach classification model was trained using a pipeline that included preprocessing and SMOTE to handle class imbalance. The models were evaluated on the validation set using key metrics:\n\n* F1 Score\n\n* Precision\n\n* Recall\n\nAll results were stored, ranked by F1 Score, and displayed in a comparison table to identify the best-performing model.","metadata":{}},{"cell_type":"code","source":"#Results\n\nresults = []\n\nfor name,model in models.items():\n    print(f\"\\n>>> Training model: {name}\")\n    print(f\"Model type: {type(model)}\")\n    final_pipeline = Pipeline([\n        ('preprocess',preprocessor),\n        ('smote', SMOTE(random_state=42)),\n        ('model', model)\n    ])\n    final_pipeline.fit(X_train,y_train)\n    pred = final_pipeline.predict(X_val)\n    results.append({\n        \"Model\":name,\n        \"F1 Score\":f1_score(y_val,pred),\n        \"Precision Score\":precision_score(y_val,pred),\n        \"Recall Score\":recall_score(y_val,pred)\n    })\n\nresults_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False)\ndisplay(results_df.reset_index(drop=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üõ†Ô∏è Hyperparameter Tuning\n\nTo improve model performance, RandomizedSearchCV was used to tune key hyperparameters for three models:\n\n* Gradient Boosting\n\n* XGBoost\n\n* Light GBM\n\nEach model was wrapped in a pipeline with preprocessing and SMOTE. Randomized search was run with 3-fold cross-validation, optimizing for F1 Score. The best parameters were selected and used to update the models for final evaluation.\n\n","metadata":{}},{"cell_type":"code","source":"# Hyperparameter Tuning\n\nparam_distributions = {\n    \"GradientBoost\": {\n        \"model__n_estimators\": randint(50, 300),\n        \"model__max_depth\": randint(3, 10),\n        \"model__learning_rate\": uniform(0.01, 0.3),\n        \"model__subsample\": uniform(0.7, 0.3)\n    },\n    \"XGBoost\": {\n        \"model__n_estimators\": randint(50, 300),\n        \"model__max_depth\": randint(3, 10),\n        \"model__learning_rate\": uniform(0.01, 0.3),\n        \"model__subsample\": uniform(0.7, 0.3),\n        \"model__colsample_bytree\": uniform(0.7, 0.3)\n    },\n    \"LightGBM\": {\n        'model__n_estimators': randint(100, 500),\n        'model__max_depth': randint(3, 10),\n        'model__learning_rate': uniform(0.01, 0.3),\n        'model__num_leaves': randint(20, 100),\n        'model__min_child_samples': randint(10, 100),\n        'model__subsample': uniform(0.7, 0.3),\n        'model__colsample_bytree': uniform(0.7, 0.3)\n    }\n}\n\n\nmodels_to_tune = [\"GradientBoost\", \"XGBoost\", \"LightGBM\"]\n\nfor name in models_to_tune:\n    print(f\"Tuning {name}...\")\n    \n    pipeline = Pipeline([\n        ('preprocess', preprocessor),\n        ('smote', SMOTE(random_state=42)),\n        ('model', models[name])\n    ])\n    \n    search = RandomizedSearchCV(\n        pipeline,\n        param_distributions[name],\n        n_iter=20,\n        cv=3,\n        scoring='f1',\n        verbose=1,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    search.fit(X_train, y_train)\n    best_pipeline = search.best_estimator_\n\n    models[name] = best_pipeline.named_steps['model']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üß™ Tuned Model Evaluation\nAfter tuning, each optimized model was retrained on the training data and evaluated on the validation set using a consistent pipeline (preprocessing + SMOTE + model).\n\nFor each tuned model (Gradient Boost, XGBoost, Light GBM), the following metrics were computed:\n\n* Precision\n\n* Recall\n\n* F1 Score\n\nThe results were tabulated and sorted by F1 Score to identify the best-performing model after hyperparameter optimization.","metadata":{}},{"cell_type":"code","source":"tuned_results = []\n\nfor name in models_to_tune:\n    \n    tuned_model = models[name]\n\n    tuned_pipeline = Pipeline([\n        ('preprocess', preprocessor),\n        ('smote', SMOTE(random_state=42)),\n        ('model', tuned_model)\n    ])\n\n    tuned_pipeline.fit(X_train, y_train)\n    y_val_pred = tuned_pipeline.predict(X_val)\n\n    precision = precision_score(y_val, y_val_pred)\n    recall = recall_score(y_val, y_val_pred)\n    f1 = f1_score(y_val, y_val_pred)\n\n    tuned_results.append({\n        \"Model\": name,\n        \"F1 Score\": f1,\n        \"Precision\": precision,\n        \"Recall\": recall,\n    })\n\nresults_df = pd.DataFrame(tuned_results).sort_values(by=\"F1 Score\", ascending=False).reset_index(drop=True)\ndisplay(results_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üßæ Final Test Prediction\nTo generate predictions for the test set:\n\nTraining Data Recombined: The training and validation sets (X_train, X_val, y_train, y_val) were merged into a single dataset (X_full, y_full) to utilize all available labeled data.\n\n* Best Model Selection: The model with the highest F1 Score from the tuning phase was selected as the final model.\n\n* Pipeline Assembly: A complete pipeline was built with:\n\n* Preprocessing (preprocessor)\n\n* SMOTE for handling class imbalance\n\n* The best-performing tuned model\n\n* Model Training & Prediction:\n\n- The pipeline was trained on the full data.\n\n- Predictions were generated on the unseen test dataset (X_test).","metadata":{}},{"cell_type":"code","source":"#Test Prediction\n\nfinal_results = []\n\nX_full = pd.concat([X_train, X_val], axis=0).reset_index(drop=True)\ny_full = pd.concat([y_train, y_val], axis=0).reset_index(drop=True)\n\n\nbest_model_name = results_df.loc[0, 'Model']\nprint(f\"\\n Best tuned model: {best_model_name}\")\n\nbest_model = models[best_model_name]\nbest_pipeline = Pipeline([\n    ('preprocess', preprocessor),\n    ('smote', SMOTE(random_state=42)),\n    ('model', best_model)\n])\n\nbest_pipeline.fit(X_full, y_full)\ny_test_pred = best_pipeline.predict(X_test)\n\nfinal_results.append({\n        \"Model\": best_model_name,\n        \"F1 Score\": f1_score(y_test, y_test_pred),\n        \"Precision\": precision_score(y_test, y_test_pred),\n        \"Recall\": recall_score(y_test, y_test_pred),\n})\n\nfinal_results_df = pd.DataFrame(final_results)\ndisplay(final_results_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}